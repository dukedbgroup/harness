Launching the Hadoop Cluster
----------------------------

# NOTE: All commands here will be run from ${AWS_HADOOP_HARNESS_HOME}
cd ${AWS_HADOOP_HARNESS_HOME} 

# Launch a Hadoop cluster: 1 Master + N Slaves 
${HADOOP_EC2_HOME}/hadoop-ec2 launch-cluster test-hadoop-cluster <number of slaves> <node type on EC2>
# Example: Launch a Hadoop cluster with 1 Master + 2 Slaves; all of the m1.small type
${HADOOP_EC2_HOME}/hadoop-ec2 launch-cluster test-hadoop-cluster 2 m1.small

# Describe the instances (used to get public ip address etc)
${EC2_HOME}/bin/ec2-describe-instances 

# Access the JobTracker web page at:
http://<replace with public domain name of Hadoop Master>:50030

# Start the proxy for Ganglia
${HADOOP_EC2_HOME}/hadoop-ec2 proxy test-hadoop-cluster &

# Login to the EC2 Hadoop Master node
${HADOOP_EC2_HOME}/hadoop-ec2 login test-hadoop-cluster

# Copy a file/dir from the local machine to the Hadoop Master Node
${HADOOP_EC2_HOME}/hadoop-ec2 push test-hadoop-cluster /local/path/to/file

# Copy a file/dir from the Hadoop Master Node to the local machine
${HADOOP_EC2_HOME}/hadoop-ec2 pull test-hadoop-cluster /master/path/to/file

# Terminate the cluster and release the EC2 nodes (run from local machine)
# *** Enter yes when the command asks for confirmation ***
${HADOOP_EC2_HOME}/hadoop-ec2 terminate-cluster test-hadoop-cluster

