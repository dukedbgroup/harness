--------------------------------------------------------------

Useful references to browse through before you start. 
Skimming through these references is recommended. You
are not required to master the details. 
----------------------------------------------------

# Getting started with Amazon Web Services (AWS)
  http://docs.amazonwebservices.com/AWSEC2/latest/GettingStartedGuide/

# Some notes on running Hadoop on AWS
  http://wiki.apache.org/hadoop/AmazonEC2

--------------------------------------------------------------
--------------------------------------------------------------

Software to be installed on local machine: 
------------------------------------------

1. ec2-api-tools
See: http://aws.amazon.com/developertools/351

2. Get the AWS harness source (harness.git) from the Git repository. See:
   https://wiki.duke.edu/display/hadoop/Source+Code
    
   # The command will be something of the form, with $USER replaced as needed:
   git clone ssh://$USER@linux.cs.duke.edu/usr/project/shivnath/git/harness.git   

   The above command will place the harness sources in the harness directory
   -- The Hadoop ec2 contrib sources we need are in: harness/hadoop_ec2_contrib_bin
   -- The AWS harness sources we need are in: harness/aws_hadoop_harness

---------------------------------------------------------------------------
----------------------------------------------------------------------------

Variables that need to be set on the local machine.
Add these to ${HOME}/.bash_profile or ${HOME}/.bashrc or on the
Duke department Linux machines to ${HOME}/.my-bash_profile since the 
${HOME}/.bash_profile does a "source ~/.my-bash_profile"
--------------------------------------------------

# ec2-api-tools that you downloaded 
export EC2_HOME=[PATH TO ec2-api-tools-<version> directory]

# Ensure the $JAVA_HOME/bin contains the java executable
export JAVA_HOME=[FILL IN with path to Java 1.6+ home]

# the ec2 contrib directory in Hadoop
export HADOOP_EC2_HOME=[FILL IN with path to harness/hadoop_ec2_contrib_bin]

# the directory containing the harness  
export AWS_HADOOP_HARNESS_HOME=[FILL IN with path to harness/aws_hadoop_harness]

# Update the paths so that we can access all the ec2-api and Hadoop contrib
#   executables from ${AWS_HADOOP_HARNESS_HOME}
export PATH=${PATH}:${JAVA_HOME}/bin:${EC2_HOME}/bin:${HADOOP_EC2_HOME}

# Variables from AWS: Get these from your AWS Security Credentials
#  in your AWS account

# Fill your 12 digit AWS Account ID here (without any "-" in between)
export AWS_USER_ID=[FILL IN]
# Get this from the Access Credentials part of your AWS Security Credentials
export AWS_ACCESS_KEY_ID=[FILL IN]
# Get this from the Access Credentials part of your AWS Security Credentials
export AWS_SECRET_ACCESS_KEY=[FILL IN]

# Note: to get the two files referred to below, you will have to 
#    create X.509 certificates using the Access Credentials part
#    of your AWS Security Credentials in your AWS account.
#    Also, it is a good idea to put the files in a standard directory 
#    like ${HOME}/.ec2  For safety, chmod this directory to have 
#    rwx------ permission (i.e., chmod 700 path/to/file) 
export EC2_PRIVATE_KEY=[FILL IN with private key file name, i.e., the file that starts with "pk-" and ends with .pem]
export EC2_CERT=[FILL IN with certificate file name, i.e., the file that starts with "cert-" and ends with .pem]

--------------------------------------------------------------
----------------------------------------------------------------------------

Create a keypair to use for the hadoop cluster. You can create it 
directly from the AWS Management Console or you could use 
${EC2_HOME}/bin/ec2-add-keypair 

Copy the template file with the local settings
cp ${HADOOP_EC2_HOME}/local_ec2_settings.sh.template ${HADOOP_EC2_HOME}/local_ec2_settings.sh

Modify the file ${HADOOP_EC2_HOME}/local_ec2_settings.sh to specify your own 
settings and credentials:
  KEY_NAME - Name of your EC2 keypair name
  PRIVATE_KEY_PATH - The full path to the EC2 keypair file
  INSTANCE_TYPE - Supported types: m1.small, m1.large, m1.xlarge, c1.medium, c1.xlarge, cc1.4xlarge
  HADOOP_VERSION - Supported versions: less than 0.19.0, 0.20.2, 0.20.203.0
  AMI_IMAGE_32 - Will be selected if INSTANCE_TYPE is m1.small or c1.medium
  AMI_IMAGE_64 - Will be selected if INSTANCE_TYPE is m1.large or m1.xlarge or c1.xlarge

************************************************
NOTE: A common mistake that happens here is not 
setting PRIVATE_KEY_PATH to the full path (including
file name).
************************************************

--------------------------------------------------------------
----------------------------------------------------------------------------

Launching the Hadoop Cluster
--------------------------------------

NOTE: All commands here will be run from the ${AWS_HADOOP_HARNESS_HOME} 

cd ${AWS_HADOOP_HARNESS_HOME} 

# Launch a Hadoop cluster: 1 Master + N Slaves. The number 
#  N is specified in the command below (we use 2 as an example)
${HADOOP_EC2_HOME}/hadoop-ec2 launch-cluster test-hadoop-cluster 2

************************************************
If you see an error of the form: 
...
Warning: Identity file /home/foo/.ec2/my-keypair not accessible: No such file or directory.
Permission denied (publickey,gssapi-with-mic).
...

Then, a likely problem is that PRIVATE_KEY_PATH is not set to the 
full and correct path to the EC2 key-pair file. If you created the key-pair 
using the AWS Management interface, then the file will have a .pem 
extension. This extension has to be included in the full path.
************************************************

# Optionally, you may specify the instance type. Note that this will
# overwrite the INSTANCE_TYPE setting in ${HADOOP_EC2_HOME}/hadoop-ec2-env.sh
${HADOOP_EC2_HOME}/hadoop-ec2 launch-cluster test-hadoop-cluster 2 m1.small

NOTE: there may be some errors shown related to Ganglia that you can ignore.
These errors happen if Ganglia is not present on the AMI being used

# CHECK OF SUCCESSFUL EXECUTION: you can access the JobTracker web page at:
http://<replace with public domain name of Hadoop Master>:50030

   1. # Note: you can find the public domain name of the Hadoop master
      # in one of two ways:
      #   A. It is printed out as HADOOP_MASTER_NODE in the console
             output when the cluster is launched. 
      #   B. Using the AWS management console or by running the following 
      #      command on the local machine and looking for the 
      #      running (not terminated) instance listed under 
      #      test-hadoop-cluster-master
      ${EC2_HOME}/bin/ec2-describe-instances 

   2. # Ensure that N slaves have joined the Hadoop cluster by checking 
      # the "Nodes" column in the Cluster Summary section right 
      # at the top part of the JobTracker's main page. Please note
      # that it will take few tens of seconds for all slaves to join the cluster. 
      # Keep refreshing the JobTracker web page for about a minute. 

   3. # If you don't see N slaves having joined, then it could be that automatic
      # start of Hadoop on all slaves did not work. Do a manual start by
      # running the following command on the Hadoop Master on EC2:
      /root/start_hadoop_on_slaves.sh
      # After that, perform the check in Step 2 to see whether all N slaves have 
      #  joined the cluster.

The launch-cluster command, if successful, will also print out the instance IDs 
and private IPs of the 1 Master and the N slaves. In addition, we capture
the AWS private IPs of the slaves in the file called SLAVE_NAMES.txt
that gets copied to the Hadoop master node. 

# Login to the EC2 Hadoop Master node. This command will get you 
# to the root prompt on the Hadoop Master if all the setup
# and configuration has been done correctly 
${HADOOP_EC2_HOME}/hadoop-ec2 login test-hadoop-cluster

# Ensure that Hadoop has been started properly on AWS by running 
#  some hadoop commands on the Hadoop Master Node on AWS.
# NOTE: Run the ${HADOOP_HOME}/bin/hadoop commands on the Hadoop Master Node

# This command should only show few HDFS files that all start with /mnt
# If this command prints non-HDFS files/directories on the Master node, 
#   then there is some problem -- somehow Hadoop got started in local mode
${HADOOP_HOME}/bin/hadoop fs -lsr /

# This command will show all the slaves. Note: it may take few tens of seconds for 
#   all slaves to register with the master. The following command should show
#   N alive workers -- i.e., a line of the following form, with N replaced with its value
#  Datanodes available: N (N total, 0 dead)
${HADOOP_HOME}/bin/hadoop dfsadmin -report

# logout from the Hadoop Master and come back to the local machine. 
#  We have to copy files from the local machine to the Hadoop Master first
logout 


---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

Configure the cluster
---------------------
${HADOOP_EC2_HOME}/hadoop-ec2-init contains a list of scripts that will be
used to initialize the cluster (i.e. set parameters like number of map/reduce
slots, HDFS block size, etc.). The script matching the INSTANCE type will
be used. For example, if your INSTANCE_TYPE is c1.medium, then
${HADOOP_EC2_HOME}/hadoop-ec2-init/hadoop-ec2-init-0.20.0-c1.medium.sh
will be used to initialize the cluster.


---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

Setup on the Hadoop Master
--------------------------

# Login to the EC2 Hadoop Master node. This command will get you 
# to the root prompt on the Hadoop Master if all the setup
# and configuration has been done correctly 
${HADOOP_EC2_HOME}/hadoop-ec2 login test-hadoop-cluster

# Create the input for Terasort. Note the two numbers in this example command:
#   -- The mapred.map.tasks=100 asks for 100 mappers to be used. If you do not 
#        specify this parameter, teragen defaults to 2 mappers and would be painfully slow. 
#   -- The 10000000 parameter specifies an input size of 10000000 records. Each record is 
#       100 bytes, so the total size is 100x10000000 == 1GB. Vary the number of records 
#       to get the desired input size
${HADOOP_HOME}/bin/hadoop jar ${HADOOP_HOME}/hadoop-*-examples.jar teragen -Dmapred.map.tasks=100 10000000 /user/shivnath/tera/in

# Example command to generate a 50GB sort input
# ${HADOOP_HOME}/bin/hadoop jar ${HADOOP_HOME}/hadoop-*-examples.jar teragen -Dmapred.map.tasks=500 500000000 /user/shivnath/tera/in

# NOTE: the /user/shivnath/tera/in HDFS output directory is defined in the exper.sh script
#   as the input directory for the Terasort experiments. If you change this path, don't 
#   forget to change the following declaration in exper.sh
#
#   declare HDFS_INPUT_DIR="/user/shivnath/tera/in"

# Confirm that the sort output has been created --- there will be one file per map task
${HADOOP_HOME}/bin/hadoop fs -lsr /user/shivnath/tera/in


---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

Shutting down the EC2 Hadoop Cluster
------------------------------------

# Once the execution is done, copy from the Hadoop cluster whatever data 
#  you want to retain. You can use the following command to 
#  pull a file/dir from the Hadoop Master Node to the local machine.
#  Run this command on the local machine. 
${HADOOP_EC2_HOME}/hadoop-ec2 pull test-hadoop-cluster /master/path/to/file

# Terminate the cluster and release the EC2 nodes (run from local machine)
${HADOOP_EC2_HOME}/hadoop-ec2 terminate-cluster test-hadoop-cluster
-- enter yes

---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

