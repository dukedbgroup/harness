--------------------------------------------------------------

Useful references before you start: 
----------------------------------
- http://docs.amazonwebservices.com/AWSEC2/latest/GettingStartedGuide/
- http://wiki.apache.org/hadoop/AmazonEC2

--------------------------------------------------------------
--------------------------------------------------------------

Software to be installed on local machine: 
------------------------------------------

1. ec2-api-tools
See: http://developer.amazonwebservices.com/connect/entry.jspa?externalID=351

2. Get the AWS harness source from the Git repository. See:
   https://wiki.duke.edu/display/hadoop/Source+Code
    
   # The command will be something of the form, with $USER replaced as needed:
   git clone ssh://$USER@linux.cs.duke.edu/usr/research/proj/git/harness.git   

   # If you are on a Duke machine with access to /usr/research/proj, the following will also work:
   git clone file:///usr/research/proj/git/harness.git   

   The above command will place the harness sources in the harness directory
   -- The Hadoop ec2 contrib sources we need are in: harness/hadoop_ec2_contrib_bin
   -- The AWS harness sources we need are in: harness/aws_hadoop_harness

---------------------------------------------------------------------------
----------------------------------------------------------------------------

Variables that need to be set on the local machine.
Add these to ${HOME}/.bash_profile or ${HOME}/.bashrc or on the
Duke department machines to ${HOME}/.my-bash_profile since the 
${HOME}/.bash_profile does a "source ~/.my-bash_profile"
--------------------------------------------------

# ec2-api-tools that you downloaded 
export EC2_HOME=[PATH TO ec2-api-tools-<version> directory]

# Ensure the $JAVA_HOME/bin contains the java executable
export JAVA_HOME=[FILL IN with path to Java 1.6+ home]

# the ec2 contrib directory in Hadoop
export HADOOP_EC2_HOME=[FILL IN with path to harness/hadoop_ec2_contrib_bin]

# the directory containing the harness  
export AWS_HADOOP_HARNESS_HOME=[FILL IN with path to harness/aws_hadoop_harness]

# Update the paths so that we can access all the ec2-api and Hadoop contrib
#   executables from ${AWS_HADOOP_HARNESS_HOME}
export PATH=${PATH}:${JAVA_HOME}/bin:${EC2_HOME}/bin:${HADOOP_EC2_HOME}

# Variables from AWS
export AWS_USER_ID=[FILL IN]

export AWS_ACCESS_KEY_ID=[FILL IN]

export AWS_SECRET_ACCESS_KEY=[FILL IN]

#  Note: it is a good idea to put the keys in 
#    a standard directory like ${HOME}/.ec2
#    For safety, chmod this directory to have rwx------ (i.e., chmod 700) permission
export EC2_PRIVATE_KEY=[FILL IN with private key file path]
export EC2_CERT=[FILL IN certificate file]

--------------------------------------------------------------
----------------------------------------------------------------------------

Create a keypair to use for the hadoop cluster. You could use 
${EC2_HOME}/bin/ec2-add-keypair 

Note: suppose you create a keypair named my-keypair
Then, the Hadoop EC2 configuration ${HADOOP_EC2_HOME}/hadoop-ec2-env.sh 
makes two assumptions 

****************** Caveat 1 as noted later *********************
1. that the private key file for this key is named as id_rsa-my-keypair
2. this private key file is stored in the same *directory* where you placed 
    your AWS private key (i.e., the ${EC2_PRIVATE_KEY} above)
****************** Caveat 1 as noted later *********************

If you want to follow a Ned's convention (as noted in 
${HADOOP_EC2_HOME}/hadoop-ec2-env.sh), you can create the file
${HADOOP_EC2_HOME}/local_ec2_settings.sh and add the two lines:

KEY_NAME="${EC2_KEYPAIR_NAME}"
PRIVATE_KEY_PATH=`echo "$EC2_KEYDIR"/"$KEY_NAME"`

You may add other settings in ${HADOOP_EC2_HOME}/local_ec2_settings.sh
to overwrite the defaults ones: 
INSTANCE_TYPE - Supported types: m1.small, m1.large, m1.xlarge, c1.medium, c1.xlarge
HADOOP_VERSION - Supported versions: 0.20.0, less than 0.19.0
AMI_IMAGE_32 - Will be selected if INSTANCE_TYPE is m1.small or c1.medium
AMI_IMAGE_64 - Will be selected if INSTANCE_TYPE is m1.large or m1.xlarge or c1.xlarge

--------------------------------------------------------------
----------------------------------------------------------------------------

Launching the Hadoop Cluster
--------------------------------------

NOTE: All commands here will be run from the ${AWS_HADOOP_HARNESS_HOME} 

cd ${AWS_HADOOP_HARNESS_HOME} 

# Launch a Hadoop cluster: 1 Master + N Slaves. The number 
#  N is specified in the command below (we use 2 as an example)
${HADOOP_EC2_HOME}/hadoop-ec2 launch-cluster test-hadoop-cluster 2

# Optionally, you may specify the instance type. Note that this will
# overwrite the INSTANCE_TYPE setting in ${HADOOP_EC2_HOME}/hadoop-ec2-env.sh
${HADOOP_EC2_HOME}/hadoop-ec2 launch-cluster test-hadoop-cluster 2 m1.small

NOTE: there may be some errors shown related to Ganglia that you can ignore.
These errors happen if Ganglia is not present on the AMI being used

# CHECK OF SUCCESSFUL EXECUTION: you can access the JobTracker web page at:
http://<replace with public domain name of Hadoop Master>:50030

   1. # Note: you can find the public domain name of the Hadoop master
      #   using the AWS management console or by running the following 
      #   command on the local machine and looking for the instance listed under 
      #   test-hadoop-cluster-master
      ${EC2_HOME}/bin/ec2-describe-instances 

   2. # Ensure that N slaves have joined the Hadoop cluster by checking 
      # the "Nodes" column in the Cluster Summary section right 
      # at the top part of the JobTracker's main page. Please note
      # that it will take few tens of seconds for all slaves to join the cluster. 
      # Keep refreshing the JobTracker web page for about a minute. 

   3. # If you don't see N slaves having joined, then it could be that automatic
      # start of Hadoop on all slaves did not work. Do a manual start by
      # running the following command on the Hadoop Master on EC2:
      /root/start_hadoop_on_slaves.sh
      # After that, perform the check in Step 2 to see whether all N slaves have 
      #  joined the cluster.

The launch-cluster command, if successful, will also print out the instance IDs 
and private IPs of the 1 Master and the N slaves. In addition, we capture:
  -- the instance ids of the slaves in the file called SLAVE_INSTANCE_IDS.txt
  -- the AWS private IPs of the slaves in the file called SLAVE_NAMES.txt
Both these files are in ${AWS_HADOOP_HARNESS_HOME} 

# Ensure that the SLAVE_NAMES look okay and that there are N of them
cat SLAVE_NAMES.txt
cat SLAVE_NAMES.txt | wc -l


# Login to the EC2 Hadoop Master node. This command will get you 
# to the root prompt on the Hadoop Master if all the setup
# and configuration has been done correctly 
${HADOOP_EC2_HOME}/hadoop-ec2 login test-hadoop-cluster

# Ensure that Hadoop has been started properly on AWS using commands like: 
# NOTE: Run the ${HADOOP_HOME}/bin/hadoop commands on the Hadoop Master Node

# This command should only show few HDFS files that all start with /mnt
# If this command prints non-HDFS files/directories on the Master node, 
#   then there is some problem -- somehow Hadoop got started in local mode
${HADOOP_HOME}/bin/hadoop fs -lsr /

# This command will show all the slaves. Note: it may take few tens of seconds for 
#   all slaves to register with the master. The following command should show
#   N alive workers -- i.e., a line of the following form, with N replaced with its value
#  Datanodes available: N (N total, 0 dead)
${HADOOP_HOME}/bin/hadoop dfsadmin -report

# logout from the Hadoop Master and come back to the local machine. 
#  We have to copy files from the local machine to the Hadoop Master first
logout 


---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

Configure the cluster
---------------------
${HADOOP_EC2_HOME}/hadoop-ec2-init contains a list of scripts that will be
used to initialize the cluster (i.e. set parameters like number of map/reduce
slots, HDFS block size, etc.). The script matching the INSTANCE type will
be used. For example, if your INSTANCE_TYPE is c1.medium, then
${HADOOP_EC2_HOME}/hadoop-ec2-init/hadoop-ec2-init-0.20.0-c1.medium.sh
will be used to initialize the cluster.


---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

Setup on the Hadoop Master
--------------------------

# Login to the EC2 Hadoop Master node. This command will get you 
# to the root prompt on the Hadoop Master if all the setup
# and configuration has been done correctly 
${HADOOP_EC2_HOME}/hadoop-ec2 login test-hadoop-cluster

# Create the input for Terasort. Note the two numbers in this example command:
#   -- The mapred.map.tasks=100 asks for 100 mappers to be used. If you do not 
#        specify this parameter, teragen defaults to 2 mappers and would be painfully slow. 
#   -- The 10000000 parameter specifies an input size of 10000000 records. Each record is 
#       100 bytes, so the total size is 100x10000000 == 1GB. Vary the number of records 
#       to get the desired input size
${HADOOP_HOME}/bin/hadoop jar ${HADOOP_HOME}/hadoop-*-examples.jar teragen -Dmapred.map.tasks=100 10000000 /user/shivnath/tera/in

# Example command to generate a 50GB sort input
# ${HADOOP_HOME}/bin/hadoop jar ${HADOOP_HOME}/hadoop-*-examples.jar teragen -Dmapred.map.tasks=500 500000000 /user/shivnath/tera/in

# NOTE: the /user/shivnath/tera/in HDFS output directory is defined in the exper.sh script
#   as the input directory for the Terasort experiments. If you change this path, don't 
#   forget to change the following declaration in exper.sh
#
#   declare HDFS_INPUT_DIR="/user/shivnath/tera/in"

# Confirm that the sort output has been created --- there will be one file per map task
${HADOOP_HOME}/bin/hadoop fs -lsr /user/shivnath/tera/in


---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

Shutting down the EC2 Hadoop Cluster
------------------------------------

# Once the execution is done, copy from the Hadoop cluster whatever data 
#  you want to retain. You can use the following command to 
#  pull a file/dir from the Hadoop Master Node to the local machine.
#  Run this command on the local machine. 
${HADOOP_EC2_HOME}/hadoop-ec2 pull test-hadoop-cluster /master/path/to/file

# Terminate the cluster and release the EC2 nodes (run from local machine)
${HADOOP_EC2_HOME}/hadoop-ec2 terminate-cluster test-hadoop-cluster
-- enter yes

---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

