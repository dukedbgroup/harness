--------------------------------------------------------------

Useful references before you start: 
----------------------------------

- http://docs.amazonwebservices.com/AWSEC2/latest/GettingStartedGuide/
- http://wiki.apache.org/hadoop/AmazonEC2

--------------------------------------------------------------
--------------------------------------------------------------

Software to be installed on local machine: 
------------------------------------------

1. ec2-api-tools
See: http://developer.amazonwebservices.com/connect/entry.jspa?externalID=351

2. Get the AWS harness source from the Git repository. See:
   https://wiki.duke.edu/display/hadoop/Source+Code
    
   # The command will be something of the form, with $USER replaced as needed:
   git clone ssh://$USER@linux.cs.duke.edu/usr/research/proj/git/harness.git   

   # If you are on a Duke machine with access to /usr/research/proj, the following will also work:
   git clone file:///usr/research/proj/git/harness.git   

   The above command will place the harness sources in the harness directory
   -- The Hadoop ec2 contrib sources we need are in: harness/hadoop_ec2_contrib_bin
   -- The AWS harness sources we need are in: harness/aws_hadoop_harness

---------------------------------------------------------------------------
----------------------------------------------------------------------------

Variables that need to be set on the local machine.
Add these to ${HOME}/.bash_profile or ${HOME}/.bashrc or on the
Duke department machines to ${HOME}/.my-bash_profile since the 
${HOME}/.bash_profile does a "source ~/.my-bash_profile"
--------------------------------------------------

# ec2-api-tools that you downloaded 
export EC2_HOME=[PATH TO ec2-api-tools-<version> directory]

# Ensure the $JAVA_HOME/bin contains the java executable
export JAVA_HOME=[FILL IN with path to Java 1.6+ home]

# the ec2 contrib directory in Hadoop
export HADOOP_EC2_HOME=[FILL IN with path to harness/hadoop_ec2_contrib_bin]

# the directory containing the harness  
export AWS_HADOOP_HARNESS_HOME=[FILL IN with path to harness/aws_hadoop_harness]

# Update the paths so that we can access all the ec2-api and Hadoop contrib
#   executables from ${AWS_HADOOP_HARNESS_HOME}
export PATH=${PATH}:${JAVA_HOME}/bin:${EC2_HOME}/bin:${HADOOP_EC2_HOME}

# Variables from AWS
export AWS_USER_ID=[FILL IN]

export AWS_ACCESS_KEY_ID=[FILL IN]

export AWS_SECRET_ACCESS_KEY=[FILL IN]

#  Note: it is a good idea to put the keys in 
#    a standard directory like ${HOME}/.ec2
#    For safety, chmod this directory to have rwx------ (i.e., chmod 700) permission
export EC2_PRIVATE_KEY=[FILL IN with private key file path]
export EC2_CERT=[FILL IN certificate file]

--------------------------------------------------------------
----------------------------------------------------------------------------

Create a keypair to use for the hadoop cluster. You could use 
${EC2_HOME}/bin/ec2-add-keypair 

Note: suppose you create a keypair named my-keypair
Then, the Hadoop EC2 configuration ${HADOOP_EC2_HOME}/hadoop-ec2-env.sh 
makes two assumptions 

****************** Caveat 1 as noted later *********************
1. that the private key file for this key is named as id_rsa-my-keypair
2. this private key file is stored in the same *directory* where you placed 
    your AWS private key (i.e., the ${EC2_PRIVATE_KEY} above)
****************** Caveat 1 as noted later *********************

Of course, you can edit the Hadoop EC2 configuration in ${HADOOP_EC2_HOME}/hadoop-ec2-env.sh 
if you follow a different convention. Ned, for example, follows a different convention
as noted in ${HADOOP_EC2_HOME}/hadoop-ec2-env.sh 

--------------------------------------------------------------
----------------------------------------------------------------------------
	
Edit all relevant variables in ${HADOOP_EC2_HOME}/hadoop-ec2-env.sh as per:
http://wiki.apache.org/hadoop/AmazonEC2

# Your Amazon Account Number. Same as ${AWS_USER_ID}
AWS_ACCOUNT_ID=${AWS_USER_ID}

# Your Amazon AWS access key. Can define it here or 
# can comment this since it will already be defined by you above
#AWS_ACCESS_KEY_ID=

# Your Amazon AWS secret access key. Can define it here or 
# can comment this since it will already be defined by you above
#AWS_SECRET_ACCESS_KEY=

# Location of EC2 keys. Note that $EC2_PRIVATE_KEY is a variable
#   that you have defined above. I put the keys in ${HOME}/.ec2
#    so that is what my EC2_KEYDIR will translate to
EC2_KEYDIR=`dirname "$EC2_PRIVATE_KEY"`

# The EC2 key name used to launch instances.
# This will be a key pair that you create and store the private key for.
#  For example: KEY_NAME=my-keypair
KEY_NAME=[FILL IN with the name of the keypair you created]

# Where your EC2 private key is stored. Should be fine as is
#  if you paid attention to Caveat 1 above.
PRIVATE_KEY_PATH=`echo "$EC2_KEYDIR"/"id_rsa-$KEY_NAME"`	

--------------------------------------------------------------
----------------------------------------------------------------------------

Launching the Hadoop Cluster
--------------------------------------

NOTE: All commands here will be run from the ${AWS_HADOOP_HARNESS_HOME} 

cd ${AWS_HADOOP_HARNESS_HOME} 

# Launch a Hadoop cluster: 1 Master + N Slaves. The number 
#  N is specified in the command below (we use 2 as an example)
${HADOOP_EC2_HOME}/hadoop-ec2 launch-cluster test-hadoop-cluster 2

NOTE: there are some errors shown related to ganglia that you can ignore.

If the command was successful, it will print out the instance IDs and private IPs
of the 1 Master and the N slaves. In addition, we capture:
  -- the instance ids of the slaves in the file called SLAVE_INSTANCE_IDS.txt
  -- the AWS private IPs of the slaves in the file called SLAVE_NAMES.txt
Both these files are in ${AWS_HADOOP_HARNESS_HOME} 

# Ensure that the SLAVE_NAMES look okay and that there are N of them
cat SLAVE_NAMES.txt
cat SLAVE_NAMES.txt | wc -l

# another check of success: you can access the JobTracker web page at:
http://<replace with public domain name of Hadoop Master>:50030
# Note: you can find the public domain name of the Hadoop master
#   using the AWS management console or by running the following 
#   command on the local machine and looking for the instance listed under 
#   test-hadoop-cluster-master
${EC2_HOME}/bin/ec2-describe-instances 

# Login to the EC2 Hadoop Master node. This command will get you 
# to the root prompt on the Hadoop Master if all the setup
# and configuration has been done correctly 
${HADOOP_EC2_HOME}/hadoop-ec2 login test-hadoop-cluster

# Ensure that Hadoop has been started properly on AWS using commands like: 
# NOTE: Run the ${HADOOP_HOME}/bin/hadoop commands on the Hadoop Master Node

# This command should only show few HDFS files that all start with /mnt
# If this command prints non-HDFS files/directories on the Master node, 
#   then there is some problem -- somehow Hadoop got started in local mode
${HADOOP_HOME}/bin/hadoop fs -lsr /

# This command will show all the slaves. Note: it may take a couple of minutes for 
#   all slaves to register with the master. The following command should show
#   N alive workers -- i.e., a line of the form, with N replaced with its value
#  Datanodes available: N (N total, 0 dead)
${HADOOP_HOME}/bin/hadoop dfsadmin -report

# logout from the Hadoop Master and come back to the local machine. 
#  We have to copy files from the local machine to the Hadoop Master first
logout 

--------------------------------------------------------------------------------- 
---------------------------------------------------------------------------------

NOTE: these commands have to be run from the local machine

Copying our files to the Hadoop Master
--------------------------------------

Before we can start using the Hadoop cluster, we need to copy our local files 
to the cluster 

#Create a tarball (tar and gzip) of the AWS_HADOOP_HARNESS files

cd ${AWS_HADOOP_HARNESS_HOME}/..

tar -zcvf AWS_HADOOP_HARNESS.tar.gz aws_hadoop_harness

# Copy the tarball from the local machine to the Hadoop Master Node
${HADOOP_EC2_HOME}/hadoop-ec2 push test-hadoop-cluster ${AWS_HADOOP_HARNESS_HOME}/../AWS_HADOOP_HARNESS.tar.gz

---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

Setup on the Hadoop Master
--------------------------

# Login to the EC2 Hadoop Master node. This command will get you 
# to the root prompt on the Hadoop Master if all the setup
# and configuration has been done correctly 
${HADOOP_EC2_HOME}/hadoop-ec2 login test-hadoop-cluster

# Create the input for Terasort. Note the two numbers in this example command:
#   -- The mapred.map.tasks=100 asks for 100 mappers to be used. If you do not 
#        specify this parameter, teragen defaults to 2 mappers and would be painfully slow. 
#   -- The 10000000 parameter specifies an input size of 10000000 records. Each record is 
#       100 bytes, so the total size is 100x10000000 == 1GB. Vary the number of records 
#       to get the desired input size
${HADOOP_HOME}/bin/hadoop jar ${HADOOP_HOME}/hadoop-*-examples.jar teragen -Dmapred.map.tasks=100 10000000 /user/shivnath/tera/in

# Example command to generate a 50GB sort input
# ${HADOOP_HOME}/bin/hadoop jar ${HADOOP_HOME}/hadoop-*-examples.jar teragen -Dmapred.map.tasks=500 500000000 /user/shivnath/tera/in

# NOTE: the /user/shivnath/tera/in HDFS output directory is defined in the exper.sh script
#   as the input directory for the Terasort experiments. If you change this path, don't 
#   forget to change the following declaration in exper.sh
#
#   declare HDFS_INPUT_DIR="/user/shivnath/tera/in"

# Confirm that the sort output has been created --- there will be one file per map task
${HADOOP_HOME}/bin/hadoop fs -lsr /user/shivnath/tera/in

---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

Running the Experiments in the EC2 Hadoop Master Node
-----------------------------------------------------

# Note: these commands have to be run on the Hadoop Master node

tar -zxvf AWS_HADOOP_HARNESS.tar.gz

cd aws_hadoop_harness

# Set the desired parameters in config.1.xml---see the example config*.xml files for 
#  the specification format. 

# use less to view files on the Hadoop Master
less config.1.xml

# use the nano or vi editors to edit files on the Hadoop Master
#  I use nano since its commands are similar to the pico editor that comes with 
#    the pine email service. You can always use the yum package manager to 
#    install other editors
# For nano commands, see: http://mintaka.sdsu.edu/reu/nano.html
nano config.1.xml

# Create the TEMP directory
mkdir TEMP

# Run the perl program to generate the input directory for each experiment: 
perl parser.pl config.1.xml ./TEMP input_dirs.txt

# Note that the above command will create one subdirectory under ./TEMP
#   per experiment. The experiments are run in a random order 
#   to avoid side-effects. The order in which the experiments will 
#   be run is the order in input_dirs.txt
cat input_dirs.txt

# Run the bash script to run the experiments.
# Note: the combination of nohup and the final "&" ensures 
#   that the job continues to run even if you logout from the Master 
nohup ./exper.sh input_dirs.txt >&OUT.txt &

# you can monitor the job execution in the JobTracker accessible at:
http://<replace with public domain name of Hadoop Master>:50030

# Note: you can find the public domain name of the Hadoop master
#   using the AWS management console or by running the following 
#   command on the local machine and looking for the instance listed under 
#   test-hadoop-cluster-master
${EC2_HOME}/bin/ec2-describe-instances 

# get the map and reduce task running times for the experiments
#  NOTE: all data per experiment will be written to the corresponding
#     directory
./get_exper_stats.sh input_dirs.txt

# To run another set of experiments, save (or rename) the TEMP file, 
#    and repeat the steps (starting with "perl parser.pl ...") with another config.xml file

---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

Examples of commands that I use to extract/summarize information from the ouput/logs
after a set of experiments are complete
---------------------------------------------------------------------------

# create a listing of the experiment directories used in a run
find ./TEMP -type d | grep "EX" >dirs.txt

#  I usually rename the main experiment directory to the TEMP.X format so 
#    that I can use the name TEMP for a new run
find ./TEMP.1 -type d | grep "EX" >dirs.txt

# extracts the parameters and the running time --- see below on examples to 
#    sort the output for easier reading
# Also note: despite the caveats in the file, get_exper_stats will work 
#    when the directory listings provided are relative rather than absolute
./get_exper_stats.sh ./dirs.txt

# sort the output on the parameter values of interest --- in the example command below, I am assuming 
#    that the first three parameters ($1, $2, and $3 in awk) are varied, the next three remain 
#    unchanged, and the running time is the seventh field ($7 in awk)
# The awk part here seems unnecessary, but I had trouble getting the multi-field sort to work as 
#   expected when the field delimitters were "," 
# Note: in the command below, the input to sort has spaces (" ") as field delimitters
./get_exper_stats.sh ./dirs.txt | awk -F, '{print $1 " " $2 " " $3 " " $7}' | sort -n --key=1 --key=2 --key=3

---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

Shutting down the EC2 Hadoop Cluster
------------------------------------

Once the execution is done, copy from the Hadoop cluster whatever data 
you want to retain

# Terminate the cluster and release the EC2 nodes (run from local machine)
${HADOOP_EC2_HOME}/hadoop-ec2 terminate-cluster test-hadoop-cluster
-- enter yes

---------------------------------------------------------------------------------
---------------------------------------------------------------------------------
