Copying the harness files to the Hadoop Master
----------------------------------------------

NOTE: these commands have to be run from the local machine

Before we can start using the Hadoop cluster, we need to copy our local files 
to the cluster 

#Create a tarball (tar and gzip) of the AWS_HADOOP_HARNESS files

cd ${AWS_HADOOP_HARNESS_HOME}/..

tar -zcvf AWS_HADOOP_HARNESS.tar.gz aws_hadoop_harness

# Copy the tarball from the local machine to the Hadoop Master Node
${HADOOP_EC2_HOME}/hadoop-ec2 push test-hadoop-cluster ${AWS_HADOOP_HARNESS_HOME}/../AWS_HADOOP_HARNESS.tar.gz


---------------------------------------------------------------------------------
---------------------------------------------------------------------------------

Running the Experiments in the EC2 Hadoop Master Node
-----------------------------------------------------

# Note: these commands have to be run on the Hadoop Master node

tar -zxvf AWS_HADOOP_HARNESS.tar.gz

cd aws_hadoop_harness

# Set the desired parameters in config.1.xml---see the example 
# sample_configs/config*.xml files for the specification format. 

# use less to view files on the Hadoop Master
less config.1.xml

# use the nano or vi editors to edit files on the Hadoop Master
#  I use nano since its commands are similar to the pico editor that comes with 
#    the pine email service. You can always use the yum package manager to 
#    install other editors
# For nano commands, see: http://mintaka.sdsu.edu/reu/nano.html
nano config.1.xml

# Run the perl program to generate the input directory for each experiment: 
Usage: perl gen_exper.pl <XML config file for experiments> <base directory for experiments>
Example: 
perl gen_exper.pl config.1.xml BASE

##########################################################
# The above perl command will fails if you do not 
#   have the XML::Simple perl module installed. Manual
#   installation instructions for XML::Simple are given at the end 
#   of this document
##########################################################

# The "perl gen_exper.pl ..." command will create one subdirectory 
#   under <base directory>. The experiments are run in a random order 
#   to avoid side-effects. The order in which the experiments will 
#   be run is the order in the file 
#   <base directory for experiments>/RANDOMIZED_EXPERIMENT_LIST.txt
cat BASE/RANDOMIZED_EXPERIMENT_LIST.txt

# Also: you can see the experiment matrix in the file:
#    <base directory for experiments>/DESIGN.txt
cat BASE/DESIGN.txt

# Run the run_exper.sh bash script to run the experiments in the matrix
#  in the randomized order 
Usage: ./run_exper.sh <base directory for experiments>
Example: 
./run_exper.sh BASE

# Alternate use: the combination of nohup and a final "&" will ensure
#   that the process will continue to run even if you logout from the Master node
Usage: nohup ./run_exper.sh <base directory for experiments> >& OUT.txt &
Example: 
nohup ./run_exper.sh BASE >& OUT.txt &

# you can monitor the job execution in the JobTracker accessible at:
http://<replace with public domain name of Hadoop Master>:50030

# Note: you can find the public domain name of the Hadoop master
#   using the AWS management console or by running the following 
#   command on the local machine and looking for the instance listed under 
#   test-hadoop-cluster-master
${EC2_HOME}/bin/ec2-describe-instances 

# Once run_exper.sh completes: 
#-----------------------------
#
#  -- The file <base directory for experiments>/HADOOP_JOB_IDS.txt will contain
#     the Hadoop JobID for the MapReduce job submitted for each experiment
#     (the JobID will be Null if the job was not submitted due to some error)
#
#  -- Each experiment directory will contain:
#----------------------------------------------
#
#     -- the output.txt file: the output progress information produced by the job  
#
#     -- the history directory -- all history files and summary for the job  
#
#     -- the profiles directory -- JVM-level profiling info for the job (if collected
#             explicitly by setting the mapred.task.profile parameter to true)

# Use the get_exper_logs.sh script to copy the task attempt log generated by Hadoop
#  for each task attempt as part of each experiment. The task logs from each 
#  Hadoop slave (TaskTracker) node are tar gzipped into a single file, and then 
#  copied to the userlogs subdirectory in the directory for each experiment
Usage: ./get_exper_logs.sh <base directory for experiments> <list of Hadoop slave nodes>
Example: 
./get_exper_logs.sh BASE /root/SLAVE_NAMES.txt

# Running "tar -zxvf <filename>" will produce the actual task attempt dirs/files
#  for each tar gzipped file

# To run another set of experiments, repeat the above steps starting from the  
#   "perl gen_exper.pl ..." step, for example, with another XML configuration file

##########################################################

Two new files have been added to the AWS Hadoop Harness:
-------------------------------------------------------

(1) clean_and_restart_cluster.sh  -- which gives the ability to delete current HDFS contents
       and restart Hadoop and HDFS, possibly with a new Hadoop and HDFS configuration. This command 
       is invoked with no parameters. See comments at the start of the file on how to provide a new
       Hadoop and HDFS configuration. 

(2) restart_cluster.sh -- which is the same as clean_and_restart_cluster.sh except that it does not
      delete the current HDFS filesystem (useful in case we want to preserve the current data in HDFS). 
      Bear in mind that properties of the current files will not change, e.g., they will have the 
      same replication factor as before. If you want to change the replication factor, then the 
      files have to be recreated (or use hadoop fs -setrep)

##########################################################
What to do if the "perl gen_exper.pl" command fails because 
  the XML::Simple perl module is not installed:
Answer: Install XML::Simple manually by running the following commands 
        on the Master node

# use yum to install perl-CPAN
yum install -y perl-CPAN

# open a CPAN shell so that we can install the XML::Simple module
perl -MCPAN -e shell
  --> answer no (to avoid manual configuration)

# Enter the following commands in the CPAN shell.
# Note the "o" in the commands below 

cpan>o conf prerequisites_policy follow
cpan>o conf commit
cpan>install XML::Simple
cpan>exit

##########################################################
